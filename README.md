# Olist Lakehouse

Este projeto constrÃ³i um **Lakehouse** para o ecommerce [Olist](https://olist.com/home/), utilizando a **[Arquitetura Medallion](https://learn.microsoft.com/en-us/azure/databricks/lakehouse/medallion)**, que organiza os dados nas camadas **bronze**, **silver** e **gold**.

O objetivo Ã© demonstrar como estruturar um pipeline de dados moderno, desde a ingestÃ£o de dados brutos atÃ© a modelagem dimensional, pronto para anÃ¡lises e BI.

---

## ğŸ—ï¸ Arquitetura

A arquitetura segue o conceito de **Lakehouse**, que combina as melhores caracterÃ­sticas de data lakes e data warehouses.

**Camadas:**
- ğŸ¥‰ **Bronze:** dados brutos, exatamente como foram extraÃ­dos do Kaggle.
- ğŸ¥ˆ **Silver:** dados limpos, tratados e integrados.
- ğŸ¥‡ **Gold:** modelo dimensional (tabelas `dim_*` e `fct_*`), pronto para consumo analÃ­tico.

![olist_architecture_diagram.png](assets/olist_architecture_diagram.png)

---

## ğŸ”„ Pipeline de Dados

1. **IngestÃ£o:**  
   Airflow (via [Astro](https://www.astronomer.io/product/)) orquestra a extraÃ§Ã£o dos dados do [Kaggle](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce), armazena no [MinIO](https://min.io/) (Data Lake) e depois carrega no [PostgreSQL](https://www.postgresql.org/) (camada bronze).

2. **TransformaÃ§Ã£o:**  
   O [dbt](https://www.getdbt.com/) aplica as transformaÃ§Ãµes necessÃ¡rias para gerar as camadas **silver** e **gold** dentro do PostgreSQL.

3. **Consumo:**  
   Os dados na camada gold estÃ£o prontos para anÃ¡lises em ferramentas de BI, notebooks, dashboards, etc.

---

## ğŸš€ InicializaÃ§Ã£o

Clone o repositÃ³rio e execute:

```bash
git clone ...
cd olist_lakehouse
docker-compose up -d
cd astro-airflow
astro dev start
